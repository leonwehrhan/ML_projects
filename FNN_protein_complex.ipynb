{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning the difference between two protein conformational states using a feed-forward neural network"
      ],
      "metadata": {
        "id": "1w78QxdRKJ7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a recent research project (published [here](https://www.biorxiv.org/content/biorxiv/early/2024/02/26/2024.02.22.581541.full.pdf)), I have discovered two distinct conformational states of the protein-protein complex of Trypsin and fluorinated BPTI variants. In the project, I have found the differences between these two states by applying chemical intuition and thorough MD analysis. In this Notebook, I retrospectively explore the approach to learn what makes these two states different, without knowledge of the states, by using a feed forward neural network with the amino acid backbone and sidechain dihedrals of the whol protein complex as features. In a next step, the predictive power of the neural net with features removed is tested, to get an intuition about the importance of that feature for the difference between the states. The Idea for this procedure comes from [this paper](https://pubs.acs.org/doi/10.1021/acs.jctc.1c00924).\n",
        "\n",
        "The following steps are needed:\n",
        "\n",
        "1. Run simulations in both states to gather data. The simulations are started in either of the complexes and it is assumed that the systems remains in this state throughout the whole simulation. The data consists of simulation snapshots, labeled 0 or 1 for the respective state.\n",
        "2. The simulation snapshots are converted into feature vectors containing all amino acid dihedrals of the protein complex.\n",
        "3. The data is split into test- and train data and the model is built and trained.\n",
        "4. The accuracy is tested when the features of the protein amino acids are separately removed, one-by-one, to see if any of the features causes a significant drop in accuracy and is therefore especially important for the difference between the two states."
      ],
      "metadata": {
        "id": "62kTkcStKXSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "sRGIdZqEO4ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simulations yielded 17000 snapshots per state. The dihedrals of all amino acids were calculated using the mdtraj package and amounted to 1003 features. To handle discontinuities in the periodic dihedrals, the features were each processed with a sine and cosine function, yielding 2006 features."
      ],
      "metadata": {
        "id": "snUjLb7CO7ai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UDKzn-grKASQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "\n",
        "def sin_cos_df(df_file, state):\n",
        "    df = pd.read_csv(df_file)\n",
        "    df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "\n",
        "    columns = []\n",
        "    sin_cos_data = []\n",
        "\n",
        "    for x in df.columns:\n",
        "        sin_cos_data.append(np.sin(df[x]))\n",
        "        columns.append(f'{x}-sin')\n",
        "\n",
        "        sin_cos_data.append(np.cos(df[x]))\n",
        "        columns.append(f'{x}-cos')\n",
        "\n",
        "    df_new = pd.concat(sin_cos_data, axis=1)\n",
        "    df_new.columns = columns\n",
        "\n",
        "    if state == 'fully_bound':\n",
        "        df_new['State'] = np.zeros(len(df_new))\n",
        "    elif state == 'pre_bound':\n",
        "        df_new['State'] = np.ones(len(df_new))\n",
        "\n",
        "    return df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data was then split into train and test data by randomly shuffling the data and using a share of 0.25 of the data as test set. (doesn't run here, as original simulation data is only stored locally)"
      ],
      "metadata": {
        "id": "VM39sf7cX4HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "\n",
        "X = df_full.drop('State', axis=1).values\n",
        "y = df_full['State'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True, random_state=2023)"
      ],
      "metadata": {
        "id": "YOdV2-RnXGTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and test data is available on Google Drive"
      ],
      "metadata": {
        "id": "cPrOIh5DSF-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.load('drive/MyDrive/FNN_protein_complex_data/X_train.npy')\n",
        "X_test = np.load('drive/MyDrive/FNN_protein_complex_data/X_test.npy')\n",
        "\n",
        "y_train = np.load('drive/MyDrive/FNN_protein_complex_data/y_train.npy')\n",
        "y_test = np.load('drive/MyDrive/FNN_protein_complex_data/y_test.npy')"
      ],
      "metadata": {
        "id": "VJSGJqwxb9P7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME5R4RKxccf1",
        "outputId": "885dda07-09ab-4f37-8cfb-ef73df5338bd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25525, 2006]) torch.Size([25525])\n",
            "torch.Size([8509, 2006]) torch.Size([8509])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build and train the model"
      ],
      "metadata": {
        "id": "_qN6sy9Hb8Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is a feed forward neural network with one hidden layer."
      ],
      "metadata": {
        "id": "awCAQjHceMDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(2023)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2006, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        Y_pred = self.forward(X)\n",
        "        return Y_pred\n",
        "\n",
        "def fit(X, y, model, opt, loss_fn, n_epochs = 1000):\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss = loss_fn(model(X), y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Loss (Epoch {epoch}): {loss.item()}')"
      ],
      "metadata": {
        "id": "fdC-1afLdAWY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was initially trained for 100 epochs, but loss was already converged after 50 epochs, so the training process was then stopped after 50 epochs."
      ],
      "metadata": {
        "id": "iWLPpgbffU72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "loss_fn = nn.functional.cross_entropy\n",
        "opt = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "fit(X_train, y_train, net, opt, loss_fn, n_epochs=51)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgmMbldfdXQt",
        "outputId": "6c93101f-06ad-48d2-ecdb-97ba4a10cc30"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (Epoch 0): 0.693171501159668\n",
            "Loss (Epoch 10): 0.5294300317764282\n",
            "Loss (Epoch 20): 0.4086780250072479\n",
            "Loss (Epoch 30): 0.3545016646385193\n",
            "Loss (Epoch 40): 0.33295848965644836\n",
            "Loss (Epoch 50): 0.32471030950546265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test set accuracy was tested and found to be very high."
      ],
      "metadata": {
        "id": "K0k1d2N5fw4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y):\n",
        "    pred = torch.argmax(y_hat, dim=1)\n",
        "    return (pred == y).float().mean()\n",
        "\n",
        "\n",
        "accuracy(net.predict(X_test), y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vn2ycsiIf-UG",
        "outputId": "1f70ad32-ddca-4e8b-a412-0092f8c9f00d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9999)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove features"
      ],
      "metadata": {
        "id": "JnNvHjZ6dKyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we remove the features of every amino acid separately, to see if the test set accuracy drops, as this would point to a specific importance of that amino acid for differentiating between the two states."
      ],
      "metadata": {
        "id": "EcMuq_UlSSft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('drive/MyDrive/FNN_protein_complex_data/res_feat_idx.json', 'r') as f:\n",
        "    res_feat_idx = json.load(f)\n",
        "\n",
        "accuracies = {}\n",
        "\n",
        "for x in res_feat_idx:\n",
        "    idx = res_feat_idx[x]\n",
        "    X = torch.clone(X_test)\n",
        "    X[:, idx] =  X_test.mean(dim=0)[idx]\n",
        "    X[:, idx] =  X_test.mean(dim=0)[idx]\n",
        "\n",
        "    y_hat = net.predict(X)\n",
        "    acc = accuracy(y_hat, y_test)\n",
        "\n",
        "    accuracies[x] = acc\n"
      ],
      "metadata": {
        "id": "EkGSxpkchxOk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we look at the 10 amno acids with the lowest accuracy."
      ],
      "metadata": {
        "id": "umUfm01YeZxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies_sorted = dict(sorted(accuracies.items(), key=lambda x:x[1]))\n",
        "\n",
        "for i, x in enumerate(accuracies_sorted):\n",
        "    print(x, accuracies_sorted[x])\n",
        "    if i == 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIVbf4LgeDc5",
        "outputId": "18c0471c-61ce-4058-913a-ce7ff7dc8314"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLU187 tensor(0.9994)\n",
            "ARG1 tensor(0.9994)\n",
            "ARG17 tensor(0.9994)\n",
            "GLY56 tensor(0.9994)\n",
            "GLN30 tensor(0.9995)\n",
            "LEU33 tensor(0.9995)\n",
            "TYR39 tensor(0.9995)\n",
            "GLY43 tensor(0.9995)\n",
            "TYR59 tensor(0.9995)\n",
            "LYS60 tensor(0.9995)\n",
            "SER61 tensor(0.9995)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the accuracy does not drop very much at all. This is expected as the difference between the states is likely to involve multiple amino acids. Interestingly, ARG17 is in the list where the accuracy drops most (albeit very slightly), which is one of the main amino acids that will change its position during the transition between the states."
      ],
      "metadata": {
        "id": "RhbkvQXrfL3B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IyNWGHzXfv_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}